import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from sentence_transformers import SentenceTransformer, util
from tqdm import tqdm
from pathlib import Path

# === ì„¤ì • ===
BASE_DIR = Path(__file__).parent.parent

CANDIDATE_PATH = BASE_DIR / 'caption_candidates.json'
OUTPUT_PATH = BASE_DIR / 'final_caption.json'
MODEL_NAME = "microsoft/phi-2"
ADAPTER_PATH = "michiboke/phi2-lora-caption-selector"
SIM_THRESHOLD = 0.87  # ì¤‘ë³µ íŒë‹¨ ê¸°ì¤€

# === ëª¨ë¸ ë¡œë”© (phi-2 + LoRA) ===
print("ğŸ”§ Loading phi-2 + LoRA model...")
base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map="auto", torch_dtype=torch.float16)
model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH)
model.eval()

# === SBERT ë¡œë”© ===
print("ğŸ”§ Loading SBERT model...")
sbert = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")

# === ë°ì´í„° ë¡œë”© ===
with open(CANDIDATE_PATH, "r", encoding="utf-8") as f:
    data = json.load(f)

selected_captions = []

# === 1ë‹¨ê³„: phi-2ë¡œ ìë§‰ ì„ íƒ ===
print("ğŸ¯ Selecting best captions using phi-2...")
for entry in tqdm(data):
    prompt = "ì¥ë©´ ì •ë³´:\n"
    prompt += f"- ëŒ€ì‚¬: {entry.get('dialogue', '') or 'ì—†ìŒ'}\n"
    prompt += f"- OCR: {' '.join(entry.get('ocr', [])) or 'ì—†ìŒ'}\n"
    yolo_objects = ', '.join([obj.get("class", "") for obj in entry.get("yolo", []) if isinstance(obj, dict)]) or "ì—†ìŒ"
    prompt += f"- ê°ì²´: {yolo_objects}\n"
    prompt += f"- ì–¼êµ´ ì •ë³´: {entry.get('deepface', '') or 'ì—†ìŒ'}\n\n"

    for i, caption in enumerate(entry["captions"], 1):
        prompt += f"{i}. {caption}\n"
    prompt += "\nê°€ì¥ ì ì ˆí•œ ìë§‰ ë²ˆí˜¸ëŠ”?"

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        output = model.generate(**inputs, max_new_tokens=5)
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    number_str = ''.join(filter(str.isdigit, response))

    try:
        best_idx = int(number_str) - 1
        best_caption = entry["captions"][best_idx]
    except:
        best_caption = entry["captions"][0]  # fallback

    selected_captions.append({
        "image": entry["image"],
        "timestamp": entry.get("timestamp", ""),
        "caption": best_caption.strip()
    })

# === 2ë‹¨ê³„: ì¤‘ë³µ ì œê±° (SBERT ê¸°ë°˜) ===
print("ğŸ§¹ Removing duplicate captions (semantic similarity)...")
deduplicated = []
prev_embedding = None

for item in selected_captions:
    emb = sbert.encode(item["caption"], convert_to_tensor=True)
    if prev_embedding is not None:
        sim = util.cos_sim(prev_embedding, emb).item()
        if sim >= SIM_THRESHOLD:
            print(f"âš ï¸ ì¤‘ë³µ ì œê±°ë¨: {item['caption']} (ìœ ì‚¬ë„: {sim:.3f})")
            continue
    deduplicated.append(item)
    prev_embedding = emb

# === ê²°ê³¼ ì €ì¥ ===
with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
    json.dump(deduplicated, f, ensure_ascii=False, indent=2)

print(f"âœ… ìë§‰ ì„ íƒ + ì¤‘ë³µ ì œê±° ì™„ë£Œ! â†’ {OUTPUT_PATH}")
